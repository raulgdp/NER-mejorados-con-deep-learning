{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "generate integer-indexed sentences, pos-tags and named entity tags, dictionaries for converting, etc, and save as `npy` binaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from preprocessing import get_vocab, index_sents\n",
    "from embedding import create_embeddings\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-tqdm in /home/deep/.local/lib/python3.8/site-packages (2.0.1)\n",
      "Requirement already satisfied: tqdm in /home/deep/.local/lib/python3.8/site-packages (from keras-tqdm) (4.46.0)\n",
      "Requirement already satisfied: Keras in /home/deep/.local/lib/python3.8/site-packages (from keras-tqdm) (2.3.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/lib/python3/dist-packages (from Keras->keras-tqdm) (1.14.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /home/deep/.local/lib/python3.8/site-packages (from Keras->keras-tqdm) (1.1.2)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /home/deep/.local/lib/python3.8/site-packages (from Keras->keras-tqdm) (1.0.8)\n",
      "Requirement already satisfied: h5py in /home/deep/.local/lib/python3.8/site-packages (from Keras->keras-tqdm) (2.10.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /home/deep/.local/lib/python3.8/site-packages (from Keras->keras-tqdm) (1.4.1)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from Keras->keras-tqdm) (5.3.1)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /home/deep/.local/lib/python3.8/site-packages (from Keras->keras-tqdm) (1.18.4)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install keras-tqdm\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers import concatenate, Input, LSTM, Dropout, Embedding\n",
    "from keras_contrib.layers import CRF\n",
    "from keras_contrib.utils import save_load_utils\n",
    "from gensim.models import Word2Vec\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from embedding import load_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set maximum network vocabulary, test set size\n",
    "MAX_VOCAB = 25000\n",
    "TEST_SIZE = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### read ConLL2002 NER corpus from csv (first save as utf-8!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Sentence #           Word  POS    Tag\n",
      "0       Sentence: 1             La   DA  B-LOC\n",
      "1               NaN         Coruña   NC  I-LOC\n",
      "2               NaN              ,   Fc      O\n",
      "3               NaN             23    Z      O\n",
      "4               NaN            may   NC      O\n",
      "...             ...            ...  ...    ...\n",
      "369166          NaN  Río-Santander  VMI  I-ORG\n",
      "369167          NaN           6,18    Z      O\n",
      "369168          NaN           +1,1    Z      O\n",
      "369169          NaN         Dycasa   NC  B-ORG\n",
      "369170          NaN              -   Fg      O\n",
      "\n",
      "[369171 rows x 4 columns]\n",
      "369171\n"
     ]
    }
   ],
   "source": [
    "data1 = pd.read_csv('data/ner_esp_train_dataset_utf8.csv')\n",
    "data2=pd.read_csv('data/ner_esp_testa_dataset_utf8.csv')\n",
    "#data.append(data2)\n",
    "data4 = pd.concat([data1, data2], ignore_index=True, sort=False)\n",
    "#print(data4)\n",
    "data3=pd.read_csv('data/ner_esp_testb_dataset_utf8.csv')\n",
    "data = pd.concat([data3, data4], ignore_index=True, sort=False)\n",
    "print(data)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sentence: 1', 'nan', 'nan', 'nan', 'nan']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentmarks = data[\"Sentence #\"].tolist()\n",
    "sentmarks = [str(s) for s in sentmarks]\n",
    "sentmarks[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = data[\"Word\"].tolist()\n",
    "postags = data[\"POS\"].tolist()\n",
    "nertags = data[\"Tag\"].tolist()\n",
    "#print(words[2],postags[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentencias del conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_text = []\n",
    "sentence_post = []\n",
    "sentence_ners = []\n",
    "\n",
    "vocab = []\n",
    "\n",
    "this_snt = []\n",
    "this_pos = []\n",
    "this_ner = []\n",
    "#print(sentmarks[:10])\n",
    "for idx, s in enumerate(sentmarks):\n",
    "    # reset if new sent\n",
    "    if s != 'nan':\n",
    "        # edit: ONLY IF HAS TAG!\n",
    "        #print(len(this_snt))\n",
    "        if len(this_snt) > 0 and this_snt[-1] == '.':\n",
    "            if list(set(this_ner)) != ['O']:\n",
    "                sentence_text.append(this_snt[:-1])\n",
    "                #print(this_snt[:-1])\n",
    "                sentence_post.append(this_pos[:-1])\n",
    "                sentence_ners.append(this_ner[:-1])\n",
    "        this_snt = []\n",
    "        this_pos = []\n",
    "        this_ner = []\n",
    "    \n",
    "    # add to lists \n",
    "    this_snt.append(words[idx].lower())\n",
    "    this_pos.append(postags[idx])\n",
    "    this_ner.append(nertags[idx])\n",
    "    vocab.append(words[idx].lower())\n",
    "   \n",
    "    #print(this_snt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['la', 'coruña', ',', '23', 'may', '(', 'efecom', ')'], ['las', 'reservas', '\"', 'on', 'line', '\"', 'de', 'billetes', 'aéreos', 'a', 'través', 'de', 'internet', 'aumentaron', 'en', 'españa', 'un', '300', 'por', 'ciento', 'en', 'el', 'primer', 'trimestre', 'de', 'este', 'año', 'con', 'respecto', 'al', 'mismo', 'período', 'de', '1999', ',', 'aseguró', 'hoy', 'iñigo', 'garcía', 'aranda', ',', 'responsable', 'de', 'comunicación', 'de', 'savia', 'amadeus']]\n",
      "['la', 'coruña', ',', '23', 'may', '(', 'efecom', ')']\n",
      "['DA', 'NC', 'Fc', 'Z', 'NC', 'Fpa', 'NP', 'Fpt']\n",
      "['B-LOC', 'I-LOC', 'O', 'O', 'O', 'O', 'B-ORG', 'O']\n",
      "\n",
      "['las', 'reservas', '\"', 'on', 'line', '\"', 'de', 'billetes', 'aéreos', 'a', 'través', 'de', 'internet', 'aumentaron', 'en', 'españa', 'un', '300', 'por', 'ciento', 'en', 'el', 'primer', 'trimestre', 'de', 'este', 'año', 'con', 'respecto', 'al', 'mismo', 'período', 'de', '1999', ',', 'aseguró', 'hoy', 'iñigo', 'garcía', 'aranda', ',', 'responsable', 'de', 'comunicación', 'de', 'savia', 'amadeus']\n",
      "['DA', 'NC', 'Fe', 'NC', 'AQ', 'Fe', 'SP', 'NC', 'AQ', 'SP', 'NC', 'SP', 'NC', 'VMI', 'SP', 'VMN', 'DI', 'Z', 'SP', 'PN', 'SP', 'DA', 'AO', 'NC', 'SP', 'DD', 'NC', 'SP', 'NC', 'SP', 'DI', 'NC', 'SP', 'Z', 'Fc', 'VMI', 'RG', 'AQ', 'NC', 'AQ', 'Fc', 'AQ', 'SP', 'NC', 'SP', 'NC', 'AQ']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'B-LOC', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'I-ORG']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(sentence_text[:2])\n",
    "for idx, sent in enumerate(sentence_text[:2]):\n",
    "    print(sent)\n",
    "    print(sentence_post[idx])\n",
    "    print(sentence_ners[idx])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentencias del conjunto de testeo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtener vocabulario y entradas de índice\n",
    "necesitamos convertir la entrada de cadena a vectores enteros para la red de keras (la red pycrfsuite necesita cadenas, ya que extraerá vectores de características de las propias palabras).\n",
    "\n",
    "indexaremos cada palabra desde 1 de acuerdo con la frecuencia inversa (la palabra más común es 1, etc.) hasta el tamaño máximo de vocabulario. Reservaremos dos espacios, 0 para el índice PAD y MAX_VOCAB-1 para palabras fuera de vocabulario o desconocidas (OOV / UNK). Como esto es algo aburrido, lo puse en funciones externas. Los paquetes como keras y sklearn tienen herramientas más robustas para esto, pero una palabra simple: el diccionario índice funcionará bien para este experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24998\n"
     ]
    }
   ],
   "source": [
    "# text vocab dicts\n",
    "# subtract 2 for UNK, PAD\n",
    "word2idx, idx2word = get_vocab(sentence_text, MAX_VOCAB-2)\n",
    "print(len(idx2word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "# POS and NER tag vocab dicts\n",
    "pos2idx, idx2pos = get_vocab(sentence_post, len(set(postags)))\n",
    "ner2idx, idx2ner = get_vocab(sentence_ners, len(set(nertags))+2)\n",
    "print(len(ner2idx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8781\n"
     ]
    }
   ],
   "source": [
    "# index\n",
    "sentence_text_idx = index_sents(sentence_text, word2idx)\n",
    "sentence_post_idx = index_sents(sentence_post, pos2idx)\n",
    "sentence_ners_idx = index_sents(sentence_ners, ner2idx)\n",
    "#print(sentence_post_idx)\n",
    "print(len(sentence_post_idx))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## División  de los conjuntos de prueba y entrenamiento.\n",
    "Dividimos los datos de entrenamiento en datos de entrenamiento y datos de prueba. los datos de prueba se usan solo para verificar el rendimiento del modelo. Un tercer conjunto, el conjunto de validación, puede separarse de nuestros datos de entrenamiento para el ajuste de hiperparámetros, aunque si utilizamos la validación cruzada k-fold, nuestro conjunto de validación cambiará cada vez."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [i for i in range(len(sentence_text))]\n",
    "#print(sentence_post_idx)\n",
    "\n",
    "#print(train_idx)\n",
    "#test_size=TEST_SIZE\n",
    "train_idx, test_idx, X_train_pos, X_test_pos = train_test_split(indices,sentence_post_idx ,test_size=TEST_SIZE)\n",
    "#X_train_pos,X_test_pos=train_test_split(indices, sentence_post_idx1 ,test_size=0.0001)\n",
    "\n",
    "\n",
    "def get_sublist(lst, indices):\n",
    "    result = []\n",
    "    for idx in indices:\n",
    "        result.append(lst[idx])\n",
    "    return result\n",
    "\n",
    "X_train_sents = get_sublist(sentence_text_idx, train_idx)\n",
    "X_test_sents = get_sublist(sentence_text_idx, test_idx)\n",
    "y_train_ner = get_sublist(sentence_ners_idx, train_idx)\n",
    "y_test_ner = get_sublist(sentence_ners_idx, test_idx)\n",
    "#print(X_test_sents,len(X_test_sents))\n",
    "#print(sentence_ners_idx)\n",
    "#print('****************************')\n",
    "#print(y_test_ner,len(y_test_ner))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creamos word2vec embeddings para  palabras, pos-tags\n",
    "\n",
    "Se ha demostrado que el uso de vectores de incrustación pre-entrenados para inicializar la capa de incrustación ayuda a la capacitación para diversas tareas de etiquetado de secuencias, como el etiquetado de POS (Huang, Xu & Yu 2015; Ma & Hovy 2016) y el Reconocimiento de entidades con nombre para inglés (Ma & Hovy 2016 ; Lee Changki 2017) y japonés (Misawa, Taniguchi, Miura y Ohkuma 2017).\n",
    "\n",
    "Como estamos usando las etiquetas POS como entrada secundaria, también entrenaremos un espacio de incrustación para estas. utilizaremos solo los datos de entrenamiento para crear las incrustaciones. Estoy usando Gensim para esta tarea, y estoy usando una función auxiliar para ajustar el Word2Vec que guarda la incrustación y también el diccionario de vocabulario. Se vectorizan 6185  sentencias y solo una de testeo que no se usa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence embeddings\n",
    "\n",
    "train_sent_texts = [sentence_text[idx] for idx in train_idx]\n",
    "        \n",
    "w2v_vocab, w2v_model = create_embeddings(train_sent_texts,\n",
    "                       embeddings_path='embedding/text_embeddings.gensimmodel',\n",
    "                       vocab_path='embedding/text_mapping.json',\n",
    "                       size=300,\n",
    "                       workers=4,\n",
    "                       iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos embeddings\n",
    "train_post_texts = [sentence_post[idx] for idx in train_idx]\n",
    "\n",
    "w2v_pvocab, w2v_pmodel = create_embeddings(train_post_texts,\n",
    "                         embeddings_path='embedding/pos_embeddings.gensimmodel',\n",
    "                         vocab_path='embedding/pos_mapping.json',\n",
    "                         size=300,\n",
    "                         workers=4,\n",
    "                         iter=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save everything to numpy binaries for loading\n",
    "\n",
    "granted, `pickle` would probably be more suitable for a lot of these things. but over-reliance on `numpy` binaries is a bad habit i've picked up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_save(saves, names):\n",
    "    for idx, item in enumerate(saves):\n",
    "        np.save('encoded/{0}.npy'.format(names[idx]), item)\n",
    "    return\n",
    "\n",
    "saves = [\n",
    "vocab,\n",
    "sentence_text_idx,\n",
    "sentence_post_idx,\n",
    "sentence_ners_idx,\n",
    "word2idx, idx2word,\n",
    "pos2idx, idx2pos,\n",
    "ner2idx, idx2ner,\n",
    "train_idx,\n",
    "test_idx,\n",
    "X_train_sents,\n",
    "X_test_sents,\n",
    "X_train_pos,\n",
    "X_test_pos,\n",
    "y_train_ner,\n",
    "y_test_ner,\n",
    "sentence_text,\n",
    "sentence_post,\n",
    "sentence_ners]\n",
    "\n",
    "names = [\n",
    "'vocab',\n",
    "'sentence_text_idx',\n",
    "'sentence_post_idx',\n",
    "'sentence_ners_idx',\n",
    "'word2idx', 'idx2word',\n",
    "'pos2idx', 'idx2pos',\n",
    "'ner2idx', 'idx2ner',\n",
    "'train_idx',\n",
    "'test_idx',\n",
    "'X_train_sents',\n",
    "'X_test_sents',\n",
    "'X_train_pos',\n",
    "'X_test_pos',\n",
    "'y_train_ner',\n",
    "'y_test_ner',\n",
    "'sentence_text',\n",
    "'sentence_post',\n",
    "'sentence_ners']\n",
    "\n",
    "numpy_save(saves, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network hyperparameters\n",
    "MAX_LENGTH = 30\n",
    "MAX_VOCAB = 25000    # see preprocessing.ipynb\n",
    "WORDEMBED_SIZE = 300 # see data_preprocessing.ipynb\n",
    "POS_EMBED_SIZE = 300 # see data_preprocessing.ipynb\n",
    "HIDDEN_SIZE = 400    # LSTM Nodes/Features/Dimension\n",
    "BATCH_SIZE = 64\n",
    "DROPOUTRATE = 0.25\n",
    "MAX_EPOCHS = 8       # max iterations, early stop condition below\n",
    "#print(y_test_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "\n",
      "[array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 3, 3, 3, 1, 2, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 3, 1, 2, 1, 1, 1, 1, 1, 1, 1])\n",
      " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,\n",
      "       1, 1, 1, 1, 1])\n",
      " array([5, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) ...\n",
      " array([1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 9, 9,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1])\n",
      " array([2, 1, 1, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 2, 1, 1, 4, 9, 1, 1, 1,\n",
      "       1, 1, 1, 2, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 9, 9, 1, 4, 9,\n",
      "       1, 1, 1, 1, 2, 3, 3, 3, 1, 2, 1, 1, 1, 4, 9, 9, 9, 1, 1, 1, 1, 1,\n",
      "       1, 2, 3, 1, 1, 1, 1, 1])\n",
      " array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 1])]\n"
     ]
    }
   ],
   "source": [
    "# load data from npys (see preprocessing.ipynb)\n",
    "print(\"loading data...\\n\")\n",
    "vocab = list(np.load('encoded/vocab.npy',allow_pickle=True))\n",
    "sentence_text = list(np.load('encoded/sentence_text.npy',allow_pickle=True))\n",
    "sentence_post = list(np.load('encoded/sentence_post.npy',allow_pickle=True))\n",
    "sentence_ners = list(np.load('encoded/sentence_ners.npy',allow_pickle=True))\n",
    "sentence_text_idx = np.load('encoded/sentence_text_idx.npy',allow_pickle=True)\n",
    "sentence_post_idx = np.load('encoded/sentence_post_idx.npy',allow_pickle=True)\n",
    "sentence_ners_idx = np.load('encoded/sentence_ners_idx.npy',allow_pickle=True)\n",
    "word2idx = np.load('encoded/word2idx.npy',allow_pickle=True).item()\n",
    "idx2word = np.load('encoded/idx2word.npy',allow_pickle=True).item()\n",
    "pos2idx = np.load('encoded/pos2idx.npy',allow_pickle=True).item()\n",
    "idx2pos = np.load('encoded/idx2pos.npy',allow_pickle=True).item()\n",
    "ner2idx = np.load('encoded/ner2idx.npy',allow_pickle=True).item()\n",
    "idx2ner = np.load('encoded/idx2ner.npy',allow_pickle=True).item()\n",
    "train_idx = np.load('encoded/train_idx.npy',allow_pickle=True)\n",
    "test_idx = np.load('encoded/test_idx.npy',allow_pickle=True)\n",
    "X_train_sents = np.load('encoded/X_train_sents.npy',allow_pickle=True)\n",
    "X_test_sents = np.load('encoded/X_test_sents.npy',allow_pickle=True)\n",
    "X_train_pos = np.load('encoded/X_train_pos.npy',allow_pickle=True)\n",
    "X_test_pos = np.load('encoded/X_test_pos.npy',allow_pickle=True)\n",
    "y_train_ner = np.load('encoded/y_train_ner.npy',allow_pickle=True)\n",
    "y_test_ner = np.load('encoded/y_test_ner.npy',allow_pickle=True)\n",
    "print(y_test_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load embedding data\n",
    "from embedding import load_vocab\n",
    "w2v_vocab, _ = load_vocab('embedding/text_mapping.json')\n",
    "w2v_model = Word2Vec.load('embedding/text_embeddings.gensimmodel')\n",
    "w2v_pvocab, _ = load_vocab('embedding/pos_mapping.json')\n",
    "w2v_pmodel = Word2Vec.load('embedding/pos_embeddings.gensimmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Secuencias de pad\n",
    "debemos 'rellenar' nuestras secuencias de entrada y salida a una longitud fija debido a la representación de gráfico fijo de Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero-padding sequences...\n",
      "\n",
      "7463\n",
      "1318\n",
      "7463\n",
      "1318\n",
      "7463\n",
      "1318\n",
      "[[1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [5 1 1 ... 0 0 0]\n",
      " ...\n",
      " [1 1 1 ... 1 1 1]\n",
      " [2 1 1 ... 3 1 1]\n",
      " [1 1 1 ... 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# zero-pad the sequences to max length\n",
    "print(\"zero-padding sequences...\\n\")\n",
    "X_train_sents = sequence.pad_sequences(X_train_sents, maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
    "X_test_sents = sequence.pad_sequences(X_test_sents, maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
    "X_train_pos = sequence.pad_sequences(X_train_pos, maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
    "X_test_pos = sequence.pad_sequences(X_test_pos, maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
    "y_train_ner = sequence.pad_sequences(y_train_ner, maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
    "y_test_ner = sequence.pad_sequences(y_test_ner, maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
    "print(len(X_train_sents))\n",
    "print(len(X_test_sents))\n",
    "print(len(X_train_pos))\n",
    "print(len(X_test_pos))\n",
    "print(len(y_train_ner))\n",
    "print(len(y_test_ner))\n",
    "\n",
    "#print(y_train_ner)\n",
    "print(y_test_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 ... 1 1 1]\n",
      " [1 1 1 ... 0 0 0]\n",
      " [5 1 1 ... 0 0 0]\n",
      " ...\n",
      " [1 1 1 ... 1 1 1]\n",
      " [2 1 1 ... 3 1 1]\n",
      " [1 1 1 ... 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "y_ner=y_test_ner\n",
    "print(y_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the size of pos-tags, ner tags\n",
    "TAG_VOCAB = len(list(idx2pos.keys()))\n",
    "NER_VOCAB = len(list(idx2ner.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape data for CRF\n",
    "y_train_ner = y_train_ner[:, :, np.newaxis]\n",
    "y_test_ner = y_test_ner[:, :, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precargar las incrustaciones pre-entrenadas\n",
    "Como se vio en estudios previos como Ma & Hovy 2016, se ha demostrado que cargar la capa de embeddings con vectores de embeddings preentrenados mejora el rendimiento de la red. Aquí inicializamos un embeddings en ceros y luego cargamos  el embeddings desde el modelo previamente entrenado (si existe; puede que no se deba a los parámetros de Word2Vec)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adicionados 5279 vectores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-52fccc5cdf1e>:10: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  word_vector = w2v_model[word]\n"
     ]
    }
   ],
   "source": [
    "# create embedding matrices from custom pretrained word2vec embeddings\n",
    "word_embedding_matrix = np.zeros((MAX_VOCAB, WORDEMBED_SIZE))\n",
    "c = 0\n",
    "for word in word2idx.keys():\n",
    "    # get the word vector from the embedding model\n",
    "    # if it's there (check against vocab list)\n",
    "    if word in w2v_vocab:\n",
    "        c += 1\n",
    "        # get the word vector\n",
    "        word_vector = w2v_model[word]\n",
    "        # slot it in at the proper index\n",
    "        word_embedding_matrix[word2idx[word]] = word_vector\n",
    "print(\"adicionados\", c, \"vectores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adicionamos 51 vectores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-25-f6a886bde429>:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  word_vector = w2v_pmodel[word]\n"
     ]
    }
   ],
   "source": [
    "pos_embedding_matrix = np.zeros((TAG_VOCAB, POS_EMBED_SIZE))\n",
    "c = 0\n",
    "for word in pos2idx.keys():\n",
    "    # get the word vector from the embedding model\n",
    "    # if it's there (check against vocab list)\n",
    "    if word in w2v_pvocab:\n",
    "        c += 1\n",
    "        # get the word vector\n",
    "        word_vector = w2v_pmodel[word]\n",
    "        # slot it in at the proper index\n",
    "        pos_embedding_matrix[pos2idx[word]] = word_vector\n",
    "print(\"adicionamos\", c, \"vectores\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17591291961020332071\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 11916696664112116047\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 8300269692827185077\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_GPU:1\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 6883249637752363091\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 5919389792\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 2202175774815677818\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 5920683456\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 7733325557587704251\n",
      "physical_device_desc: \"device: 1, name: GeForce GTX 1060 6GB, pci bus id: 0000:02:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo BiLSTM+ CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deep/.local/lib/python3.8/site-packages/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
      "  warnings.warn('CRF.loss_function is deprecated '\n",
      "/home/deep/.local/lib/python3.8/site-packages/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
      "  warnings.warn('CRF.accuracy is deprecated and it '\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "\n",
    "# text layers : dense embedding > dropout > bi-LSTM\n",
    "txt_input = Input(shape=(MAX_LENGTH,), name='txt_input')\n",
    "txt_embed = Embedding(MAX_VOCAB, WORDEMBED_SIZE, input_length=MAX_LENGTH,\n",
    "                      weights=[word_embedding_matrix],\n",
    "                      name='txt_embedding', trainable=True)(txt_input)\n",
    "txt_drpot = Dropout(DROPOUTRATE, name='txt_dropout')(txt_embed)\n",
    "\n",
    "# pos layers : dense embedding > dropout > bi-LSTM\n",
    "pos_input = Input(shape=(MAX_LENGTH,), name='pos_input')\n",
    "pos_embed = Embedding(TAG_VOCAB, POS_EMBED_SIZE, input_length=MAX_LENGTH,\n",
    "                      weights=[pos_embedding_matrix],\n",
    "                      name='pos_embedding', trainable=True)(pos_input)\n",
    "pos_drpot = Dropout(DROPOUTRATE, name='pos_dropout')(pos_embed)\n",
    "\n",
    "# merged layers : merge (concat, average...) word and pos > bi-LSTM > bi-LSTM\n",
    "mrg_cncat = concatenate([txt_drpot, pos_drpot], axis=2)\n",
    "mrg_lstml = Bidirectional(LSTM(HIDDEN_SIZE, return_sequences=True),\n",
    "                          name='mrg_bidirectional_1')(mrg_cncat)\n",
    "\n",
    "# extra LSTM layer, if wanted\n",
    "mrg_drpot = Dropout(DROPOUTRATE, name='mrg_dropout')(mrg_lstml)\n",
    "mrg_lstml = Bidirectional(LSTM(HIDDEN_SIZE, return_sequences=True),\n",
    "                          name='mrg_bidirectional_2')(mrg_lstml)\n",
    "\n",
    "\n",
    "# final linear chain CRF layer\n",
    "crf = CRF(NER_VOCAB, sparse_target=True)\n",
    "mrg_chain = crf(mrg_lstml)\n",
    "\n",
    "model = Model(inputs=[txt_input, pos_input], outputs=mrg_chain)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=crf.loss_function,\n",
    "              metrics=[crf.accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "txt_input (InputLayer)          (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "pos_input (InputLayer)          (None, 30)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "txt_embedding (Embedding)       (None, 30, 300)      7500000     txt_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "pos_embedding (Embedding)       (None, 30, 300)      18000       pos_input[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "txt_dropout (Dropout)           (None, 30, 300)      0           txt_embedding[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "pos_dropout (Dropout)           (None, 30, 300)      0           pos_embedding[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 30, 600)      0           txt_dropout[0][0]                \n",
      "                                                                 pos_dropout[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "mrg_bidirectional_1 (Bidirectio (None, 30, 800)      3203200     concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "mrg_bidirectional_2 (Bidirectio (None, 30, 800)      3843200     mrg_bidirectional_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "crf_1 (CRF)                     (None, 30, 11)       8954        mrg_bidirectional_2[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 14,573,354\n",
      "Trainable params: 14,573,354\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "#save_load_utils.load_all_weights(model,'model/crf_model.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7463 7463 7463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deep/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      " - 31s - loss: 0.4239 - crf_viterbi_accuracy: 0.8878\n",
      "Epoch 2/30\n",
      " - 29s - loss: 0.1871 - crf_viterbi_accuracy: 0.9414\n",
      "Epoch 3/30\n",
      " - 29s - loss: 0.1218 - crf_viterbi_accuracy: 0.9581\n",
      "Epoch 4/30\n",
      " - 30s - loss: 0.0783 - crf_viterbi_accuracy: 0.9704\n",
      "Epoch 5/30\n",
      " - 29s - loss: 0.0518 - crf_viterbi_accuracy: 0.9782\n",
      "Epoch 6/30\n",
      " - 30s - loss: 0.0321 - crf_viterbi_accuracy: 0.9838\n",
      "Epoch 7/30\n",
      " - 30s - loss: 0.0162 - crf_viterbi_accuracy: 0.9884\n",
      "Epoch 8/30\n",
      " - 30s - loss: 0.0046 - crf_viterbi_accuracy: 0.9914\n",
      "Epoch 9/30\n",
      " - 30s - loss: -3.8245e-03 - crf_viterbi_accuracy: 0.9933\n",
      "Epoch 10/30\n",
      " - 30s - loss: -1.0681e-02 - crf_viterbi_accuracy: 0.9946\n",
      "Epoch 11/30\n",
      " - 30s - loss: -1.8007e-02 - crf_viterbi_accuracy: 0.9958\n",
      "Epoch 12/30\n",
      " - 30s - loss: -2.4284e-02 - crf_viterbi_accuracy: 0.9969\n",
      "Epoch 13/30\n",
      " - 30s - loss: -2.8867e-02 - crf_viterbi_accuracy: 0.9971\n",
      "Epoch 14/30\n",
      " - 30s - loss: -3.3870e-02 - crf_viterbi_accuracy: 0.9978\n",
      "Epoch 15/30\n",
      " - 30s - loss: -3.8610e-02 - crf_viterbi_accuracy: 0.9981\n",
      "Epoch 16/30\n",
      " - 30s - loss: -4.2545e-02 - crf_viterbi_accuracy: 0.9982\n",
      "Epoch 17/30\n",
      " - 30s - loss: -4.6966e-02 - crf_viterbi_accuracy: 0.9984\n",
      "Epoch 18/30\n",
      " - 29s - loss: -5.0875e-02 - crf_viterbi_accuracy: 0.9984\n",
      "Epoch 19/30\n",
      " - 29s - loss: -5.5209e-02 - crf_viterbi_accuracy: 0.9987\n",
      "Epoch 20/30\n",
      " - 30s - loss: -5.8905e-02 - crf_viterbi_accuracy: 0.9986\n",
      "Epoch 21/30\n",
      " - 30s - loss: -6.2717e-02 - crf_viterbi_accuracy: 0.9987\n",
      "Epoch 22/30\n",
      " - 30s - loss: -6.6464e-02 - crf_viterbi_accuracy: 0.9987\n",
      "Epoch 23/30\n",
      " - 29s - loss: -6.9695e-02 - crf_viterbi_accuracy: 0.9984\n",
      "Epoch 24/30\n",
      " - 30s - loss: -7.3548e-02 - crf_viterbi_accuracy: 0.9984\n",
      "Epoch 25/30\n",
      " - 30s - loss: -7.6712e-02 - crf_viterbi_accuracy: 0.9982\n",
      "Epoch 26/30\n",
      " - 30s - loss: -8.0358e-02 - crf_viterbi_accuracy: 0.9982\n",
      "Epoch 27/30\n",
      " - 29s - loss: -8.4122e-02 - crf_viterbi_accuracy: 0.9981\n",
      "Epoch 28/30\n",
      " - 30s - loss: -8.8488e-02 - crf_viterbi_accuracy: 0.9985\n",
      "Epoch 29/30\n",
      " - 30s - loss: -9.3054e-02 - crf_viterbi_accuracy: 0.9987\n",
      "Epoch 30/30\n",
      " - 29s - loss: -9.7082e-02 - crf_viterbi_accuracy: 0.9987\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_sents),len(X_train_pos),len(y_train_ner))\n",
    "history = model.fit([X_train_sents, X_train_pos], y_train_ner,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=30,\n",
    "\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_dict = history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models saved!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# save the model\n",
    "# because we are using keras-contrib, we must save weights like this, and load into network\n",
    "# (see decoding.ipynb)\n",
    "save_load_utils.save_all_weights(model, 'model/crf_model.h5')\n",
    "np.save('model/hist_dict.npy', hist_dict)\n",
    "print(\"models saved!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1318 1318\n",
      "[[   37     3  1183 ...  2801     1     3]\n",
      " [   11   833     5 ...     0     0     0]\n",
      " [24496   581   858 ...     0     0     0]\n",
      " ...\n",
      " [    3   653     1 ...    18   276     5]\n",
      " [  321    30     3 ...  3561     2     5]\n",
      " [   32   854  5786 ...   272     1 19071]]\n",
      "[[ 2  3  1 ...  1  2  3]\n",
      " [ 3  1 12 ...  0  0  0]\n",
      " [ 1 28  6 ...  0  0  0]\n",
      " ...\n",
      " [ 3  1  2 ... 10  1 12]\n",
      " [ 4 22  3 ...  4  5 12]\n",
      " [21  1  6 ...  4  2  7]]\n"
     ]
    }
   ],
   "source": [
    "print(len(X_test_sents),len(X_test_pos))\n",
    "print(X_test_sents)\n",
    "print(X_test_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "preds = model.predict([X_test_sents, X_test_pos])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1318, 30)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = np.argmax(preds, axis=-1)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1318, 30)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trues = np.squeeze(y_test_ner, axis=-1)\n",
    "trues.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_preds = [[idx2ner[t] for t in s] for s in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_trues = [[idx2ner[t] for t in s] for s in trues]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 80.5%\n"
     ]
    }
   ],
   "source": [
    "from seqeval.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "print(\"F1-score: {:.1%}\".format(f1_score(s_preds, s_trues)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "      ORG       0.77      0.80      0.78      1150\n",
      "     MISC       0.41      0.52      0.46       317\n",
      "      PER       0.89      0.90      0.89       745\n",
      "      LOC       0.85      0.78      0.82       762\n",
      "      PAD       1.00      1.00      1.00       406\n",
      "\n",
      "micro avg       0.80      0.81      0.81      3380\n",
      "macro avg       0.81      0.81      0.81      3380\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(s_trues, s_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from itertools import chain\n",
    "def bio_classification_report(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    from scrapinghub's python-crfsuite example\n",
    "    \n",
    "    Classification report for a list of BIO-encoded sequences.\n",
    "    It computes token-level metrics and discards \"O\" labels.\n",
    "    \n",
    "    Note that it requires scikit-learn 0.15+ (or a version from github master)\n",
    "    to calculate averages properly!\n",
    "    \"\"\"\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "        \n",
    "    tagset = set(lb.classes_) - {'O', 'PAD'}\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "    )\n",
    "    lb = LabelBinarizer()\n",
    "    y_true_combined = lb.fit_transform(list(chain.from_iterable(y_true)))\n",
    "    y_pred_combined = lb.transform(list(chain.from_iterable(y_pred)))\n",
    "        \n",
    "    tagset = set(lb.classes_) - {'O'}\n",
    "    tagset = sorted(tagset, key=lambda tag: tag.split('-', 1)[::-1])\n",
    "    class_indices = {cls: idx for idx, cls in enumerate(lb.classes_)}\n",
    "    \n",
    "    return classification_report(\n",
    "        y_true_combined,\n",
    "        y_pred_combined,\n",
    "        labels = [class_indices[cls] for cls in tagset],\n",
    "        target_names = tagset,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.87      0.79      0.83       762\n",
      "       I-LOC       0.79      0.70      0.74       246\n",
      "      B-MISC       0.48      0.59      0.53       317\n",
      "      I-MISC       0.64      0.52      0.57       474\n",
      "       B-ORG       0.81      0.83      0.82      1150\n",
      "       I-ORG       0.79      0.74      0.77       891\n",
      "       B-PER       0.91      0.90      0.91       745\n",
      "       I-PER       0.94      0.93      0.94       658\n",
      "\n",
      "   micro avg       0.81      0.78      0.80      5243\n",
      "   macro avg       0.78      0.75      0.76      5243\n",
      "weighted avg       0.81      0.78      0.80      5243\n",
      " samples avg       0.10      0.10      0.10      5243\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deep/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/deep/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(bio_classification_report(s_trues, s_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   37     3  1183 ...  2801     1     3]\n",
      " [   11   833     5 ...     0     0     0]\n",
      " [24496   581   858 ...     0     0     0]\n",
      " ...\n",
      " [   12    20    54 ...  6311  2788    14]\n",
      " [   15   785     1 ...   645     1   297]\n",
      " [   18   272     1 ...    21   132  1784]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test_sents[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   37     3  1183 ...  2801     1     3]\n",
      " [   11   833     5 ...     0     0     0]\n",
      " [24496   581   858 ...     0     0     0]\n",
      " ...\n",
      " [   12    20    54 ...  6311  2788    14]\n",
      " [   15   785     1 ...   645     1   297]\n",
      " [   18   272     1 ...    21   132  1784]]\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "##### hash(tuple(np.array([1,2,3,4])))\n",
    "#print(y_ner)\n",
    "print(X_test_sents[:500])\n",
    "print(len(X_test_sents[:500]))\n",
    "decoded = []\n",
    "for sent_idx in range(len(X_test_sents[:500])):\n",
    "    \n",
    "    this_txt = sequence.pad_sequences([X_test_sents[sent_idx]], maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
    "    this_pos = sequence.pad_sequences([X_test_pos[sent_idx]], maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
    "    this_pred = model.predict([this_txt, this_pos])\n",
    "    this_pred = [np.argmax(p) for p in this_pred[0]]\n",
    "    np.shape(this_pred)\n",
    "    #print(this_pred)\n",
    "    # for each word in the sentence...\n",
    "    word, pos, tru, prd = [], [], [], []\n",
    "    for idx, wordid in enumerate(X_test_sents[sent_idx][:len(this_pred)]):\n",
    "        # decode word\n",
    "        word.append(idx2word[wordid])\n",
    "        # decode pos\n",
    "        #print(X_test_pos[sent_idx][idx])\n",
    "        pos.append(idx2pos[X_test_pos[sent_idx][idx]])\n",
    "        # decode true NER tag\n",
    "        #print(pos)\n",
    "        #print(y_ner[sent_idx][idx])\n",
    "        tru.append(idx2ner[y_ner[sent_idx][idx]])\n",
    "        # decode prediction\n",
    "        #print(tru)\n",
    "        prd.append(idx2ner[this_pred[idx]])\n",
    "        #print(prd)\n",
    "    \n",
    "    answ = pd.DataFrame(\n",
    "    {\n",
    "        'word': word,\n",
    "        'pos': pos,\n",
    "        'true': tru,\n",
    "        'pred': prd,\n",
    "        'skip' : [' ' for s in word]\n",
    "    })\n",
    "    answ = answ[['word', 'pos', 'true', 'pred', 'skip']]\n",
    "    answ = answ.T\n",
    "    decoded.append(answ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <td>según</td>\n",
       "      <td>la</td>\n",
       "      <td>acusación</td>\n",
       "      <td>,</td>\n",
       "      <td>esos</td>\n",
       "      <td>albaneses</td>\n",
       "      <td>fueron</td>\n",
       "      <td>miembros</td>\n",
       "      <td>de</td>\n",
       "      <td>la</td>\n",
       "      <td>...</td>\n",
       "      <td>)</td>\n",
       "      <td>,</td>\n",
       "      <td>disuelta</td>\n",
       "      <td>y</td>\n",
       "      <td>desmilitarizada</td>\n",
       "      <td>tras</td>\n",
       "      <td>el</td>\n",
       "      <td>despliegue</td>\n",
       "      <td>de</td>\n",
       "      <td>la</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>SP</td>\n",
       "      <td>DA</td>\n",
       "      <td>NC</td>\n",
       "      <td>Fc</td>\n",
       "      <td>DD</td>\n",
       "      <td>NC</td>\n",
       "      <td>VSI</td>\n",
       "      <td>NC</td>\n",
       "      <td>SP</td>\n",
       "      <td>DA</td>\n",
       "      <td>...</td>\n",
       "      <td>Fpt</td>\n",
       "      <td>Fc</td>\n",
       "      <td>AQ</td>\n",
       "      <td>CC</td>\n",
       "      <td>VMP</td>\n",
       "      <td>SP</td>\n",
       "      <td>DA</td>\n",
       "      <td>NC</td>\n",
       "      <td>SP</td>\n",
       "      <td>DA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>true</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>...</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred</th>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>...</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>skip</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0   1          2   3     4          5       6         7   8   9   \\\n",
       "word  según  la  acusación   ,  esos  albaneses  fueron  miembros  de  la   \n",
       "pos      SP  DA         NC  Fc    DD         NC     VSI        NC  SP  DA   \n",
       "true      O   O          O   O     O          O       O         O   O   O   \n",
       "pred      O   O          O   O     O          O       O         O   O   O   \n",
       "skip                                                                        \n",
       "\n",
       "      ...   20  21        22  23               24    25  26          27  28  \\\n",
       "word  ...    )   ,  disuelta   y  desmilitarizada  tras  el  despliegue  de   \n",
       "pos   ...  Fpt  Fc        AQ  CC              VMP    SP  DA          NC  SP   \n",
       "true  ...    O   O         O   O                O     O   O           O   O   \n",
       "pred  ...    O   O         O   O                O     O   O           O   O   \n",
       "skip  ...                                                                     \n",
       "\n",
       "      29  \n",
       "word  la  \n",
       "pos   DA  \n",
       "true   O  \n",
       "pred   O  \n",
       "skip      \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
