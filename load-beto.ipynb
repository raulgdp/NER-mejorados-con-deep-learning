{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "collective-royalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../libs')\n",
    "\n",
    "import datetime, os\n",
    "import random\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tabulate import tabulate\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from utils import build_matrix_embeddings as bme, plot_model_performance, logits_to_tokens, report_to_df\n",
    "from transformers import (\n",
    "    TF2_WEIGHTS_NAME,\n",
    "    BertConfig,\n",
    "    BertTokenizer,\n",
    "    TFBertForTokenClassification,\n",
    "    create_optimizer)\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "# ****** DEFINICION DE PARAMETROS *********\n",
    "MAX_LEN        = 409\n",
    "NUM_LABELS     = 9 + 3\n",
    "WORD_PAD_TOKEN = 0\n",
    "\n",
    "ESPECIAL_TOKEN = 9\n",
    "SEP_TOKEN      = 10\n",
    "PAD_TOKEN      = 11\n",
    "\n",
    "configuration = BertConfig()\n",
    "BERT_MODEL = \"dccuchile/bert-base-spanish-wwm-cased\"\n",
    "\n",
    "save_dir      = \"./model\"\n",
    "\n",
    "le_dict = {}\n",
    "\n",
    "le_dicti = {'B-LOC': 0, 'B-MISC': 1, 'B-ORG': 2, 'B-PER': 3, 'I-LOC': 4, 'I-MISC': 5, 'I-ORG': 6, 'I-PER': 7, 'O': 8, '[CLS]': 9, '[SEP]': 10, '[PAD]': 11}\n",
    "\n",
    "for key in le_dicti:\n",
    "    #print(key, '->', le_dict[key])\n",
    "    le_dict[le_dicti[key]] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "everyday-sellers",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_input(sentences, tags, in_ou_put):\n",
    "    input_id_list       = []\n",
    "    attention_mask_list = [] \n",
    "    token_type_id_list  = []\n",
    "    \n",
    "    if in_ou_put == 1:\n",
    "        label_id_list   = []\n",
    "    else:\n",
    "        label_id_list   = 0\n",
    "    \n",
    "    for x,y in tqdm(zip(sentences,tags),total=len(tags)):\n",
    "        tokens = []\n",
    "        \n",
    "        if in_ou_put == 1:\n",
    "            label_ids = []\n",
    "        \n",
    "        for word, label in zip(x, y):\n",
    "            word_tokens = tokenizer.tokenize(str(word))\n",
    "            tokens.extend(word_tokens)\n",
    "            # Use the real label id for the first token of the word, \n",
    "            # and padding ids for the remaining tokens\n",
    "            if in_ou_put == 1:\n",
    "                #label_ids.extend([label_map[label]] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
    "                label_ids.extend([label] + [SEP_TOKEN] * (len(word_tokens) - 1))\n",
    "        \n",
    "        # special_tokens_count =  2\n",
    "        \n",
    "        #if len(tokens) > LEN_SENTS - special_tokens_count:\n",
    "        #    tokens = tokens[: (LEN_SENTS - special_tokens_count)]\n",
    "\n",
    "        #    if in_ou_put == 1:\n",
    "        #        label_ids = label_ids[: (LEN_SENTS - special_tokens_count)]\n",
    "        \n",
    "        if in_ou_put == 1:\n",
    "            #label_ids = [pad_token_label_id] + label_ids + [pad_token_label_id]\n",
    "            label_ids = [ESPECIAL_TOKEN] + label_ids + [ESPECIAL_TOKEN]\n",
    "        \n",
    "        inputs = tokenizer.encode_plus(tokens, add_special_tokens=True, max_length=MAX_LEN)\n",
    "        \n",
    "        input_ids       = inputs[\"input_ids\"]\n",
    "        token_type_ids  = inputs[\"token_type_ids\"]\n",
    "        attention_masks = inputs[\"attention_mask\"]\n",
    "        \n",
    "        #print(attention_masks)\n",
    "        #attention_masks = [17] + [1] * (len(input_ids)-2) + [17]\n",
    "        #print(attention_masks)\n",
    "        \n",
    "        attention_mask_list.append(attention_masks)\n",
    "        input_id_list.append(input_ids)\n",
    "        token_type_id_list.append(token_type_ids)\n",
    "        \n",
    "        if in_ou_put == 1:\n",
    "            label_id_list.append(label_ids)\n",
    "\n",
    "    input_id_list       = pad_sequences(maxlen=MAX_LEN, sequences=input_id_list,       dtype=\"int32\", padding=\"post\", value=WORD_PAD_TOKEN)\n",
    "    token_type_id_list  = pad_sequences(maxlen=MAX_LEN, sequences=token_type_id_list,  dtype=\"int32\", padding=\"post\")\n",
    "    attention_mask_list = pad_sequences(maxlen=MAX_LEN, sequences=attention_mask_list, dtype=\"int32\", padding=\"post\")\n",
    "    \n",
    "    if in_ou_put == 1:\n",
    "        label_id_list   = pad_sequences(maxlen=MAX_LEN, sequences=label_id_list, dtype=\"int32\", padding=\"post\", value=PAD_TOKEN)\n",
    "        #label_id_list   = [to_categorical(i, num_classes=num_labels, dtype =\"int32\") for i in label_id_list]\n",
    "        #label_id_list   = np.array(label_id_list)\n",
    "\n",
    "    return input_id_list, token_type_id_list, attention_mask_list, label_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "destroyed-administrator",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\"bert\": (BertConfig, TFBertForTokenClassification, BertTokenizer)}\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES['bert']\n",
    "config = config_class.from_pretrained(BERT_MODEL, num_labels=NUM_LABELS)\n",
    "\n",
    "tokenizer = tokenizer_class.from_pretrained(BERT_MODEL, do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "detected-thing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['La', 'bio', '##psia', 'no', 'muestra', 'células', 'de', 'cáncer', '.']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = tokenizer.tokenize(\"La biopsia no muestra células de cáncer.\")\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "innovative-presentation",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|██████████| 2/2 [00:00<00:00, 1314.21it/s]\n"
     ]
    }
   ],
   "source": [
    "negation_samples = [\n",
    "    \"Correr en Colombia con James Rodriguez .\".split(),\n",
    "    \"Éste gran hombre ganó con el Real Madrid de España y en Alemania con Roler.\".split()\n",
    "]\n",
    "\n",
    "dummy_y_train = []\n",
    "\n",
    "for snt in negation_samples:\n",
    "    senti = []\n",
    "    for wds in snt:\n",
    "        senti.append('-PAD-')\n",
    "    \n",
    "    dummy_y_train.append(senti)\n",
    "\n",
    "\n",
    "demo_input_ids_train, demo_token_ids_train, demo_attention_masks_train, label_ids_train = convert_to_input(negation_samples, dummy_y_train, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "intermediate-shell",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = tf.saved_model.load(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "criminal-stack",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_prediction = new_model([demo_input_ids_train, demo_token_ids_train, demo_attention_masks_train])\n",
    "\n",
    "demo_pred_tags = np.argmax(demo_prediction, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "figured-target",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_y_pred = logits_to_tokens(demo_pred_tags, le_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "premier-model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Correr', 'en', 'Colombia', 'con', 'James', 'Rodriguez', '.']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='overflow-x: auto; white-space: nowrap;'><table>\n",
       "<thead>\n",
       "<tr><th>Corre  </th><th>##r  </th><th>en  </th><th>Colombia  </th><th>con  </th><th>J    </th><th>##ames  </th><th>Rodri  </th><th>##guez  </th><th>.  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>O      </td><td>[SEP]</td><td>O   </td><td>B-LOC     </td><td>O    </td><td>B-PER</td><td>[SEP]   </td><td>I-PER  </td><td>[SEP]   </td><td>O  </td></tr>\n",
       "</tbody>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Éste', 'gran', 'hombre', 'ganó', 'con', 'el', 'Real', 'Madrid', 'de', 'España', 'y', 'en', 'Alemania', 'con', 'Roler.']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='overflow-x: auto; white-space: nowrap;'><table>\n",
       "<thead>\n",
       "<tr><th>É  </th><th>##ste  </th><th>gran  </th><th>hombre  </th><th>ganó  </th><th>con  </th><th>el  </th><th>Real  </th><th>Madrid  </th><th>de   </th><th>España  </th><th>y  </th><th>en  </th><th>Alemania  </th><th>con  </th><th>Rol  </th><th>##er  </th><th>.  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>O  </td><td>[SEP]  </td><td>O     </td><td>O       </td><td>O     </td><td>O    </td><td>O   </td><td>B-ORG </td><td>I-ORG   </td><td>I-ORG</td><td>I-ORG   </td><td>O  </td><td>O   </td><td>B-LOC     </td><td>O    </td><td>B-PER</td><td>[SEP] </td><td>O  </td></tr>\n",
       "</tbody>\n",
       "</table></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for h, oracc in enumerate(negation_samples):\n",
    "    #heads = oracc\n",
    "    #if h == 0:\n",
    "    tokensito = []\n",
    "    for wordi in oracc:\n",
    "        wordi_tokens = tokenizer.tokenize(str(wordi))\n",
    "        tokensito.extend(wordi_tokens)\n",
    "\n",
    "    #print(oracc)\n",
    "    #print(tokensito)\n",
    "    #print(demo_y_pred[h])\n",
    "    heads = tokensito\n",
    "    body  = [demo_y_pred[h][1:len(tokensito)+1]]\n",
    "    display(HTML(\"<div style='overflow-x: auto; white-space: nowrap;'>\" + \n",
    "                 tabulate(body, headers=heads, tablefmt=\"html\") + \n",
    "                 \"</div>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-fleece",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
